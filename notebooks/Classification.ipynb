{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The notebook is adapted from https://juliaacademy.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "[Classification](https://www.datacamp.com/blog/classification-machine-learning) is a supervised machine learning process that involves predicting the class of given data points.  Those classes can be targets, labels or categories. Put simply, classification is the task of predicting a label for a given observation. For example: you are given certain physical descriptions of an animal, and your taks is to classify them as either a dog or a cat. Here, we will classify iris flowers.\n",
    "\n",
    "As we will see later, we will use different classifiers and at the end of this notebook, we will compare them. We will define our accuracy function right now to get it out of the way. We will use a simple accuracy function that returns the ratio of the number of correctly classified observations to the total number of predictions.\n",
    "\n",
    "Common classification algorithms include: K-nearest neighbor, decision trees, naive Bayes and artificial neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "findaccuracy (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "findaccuracy(predictedvals,groundtruthvals) = sum(predictedvals.==groundtruthvals)/length(groundtruthvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.9/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.9/Manifest.toml`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.9/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.9/Manifest.toml`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.9/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.9/Manifest.toml`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.9/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.9/Manifest.toml`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.9/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.9/Manifest.toml`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.9/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.9/Manifest.toml`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.9/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.9/Manifest.toml`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.9/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.9/Manifest.toml`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.9/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.9/Manifest.toml`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.9/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.9/Manifest.toml`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.9/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.9/Manifest.toml`\n"
     ]
    }
   ],
   "source": [
    "import Pkg; Pkg.add(\"GLMNet\")\n",
    "Pkg.add(\"RDatasets\")\n",
    "Pkg.add(\"MLBase\")\n",
    "Pkg.add(\"Plots\")\n",
    "Pkg.add(\"DecisionTree\")\n",
    "Pkg.add(\"Distances\")\n",
    "Pkg.add(\"NearestNeighbors\")\n",
    "Pkg.add(\"Random\")\n",
    "Pkg.add(\"LinearAlgebra\")\n",
    "Pkg.add(\"DataStructures\")\n",
    "Pkg.add(\"LIBSVM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "using GLMNet\n",
    "using RDatasets\n",
    "using MLBase\n",
    "using Plots\n",
    "using DecisionTree\n",
    "using Distances\n",
    "using NearestNeighbors\n",
    "using Random\n",
    "using LinearAlgebra\n",
    "using DataStructures\n",
    "using LIBSVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the data first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div style = \"float: left;\"><span>150√ó5 DataFrame</span></div><div style = \"float: right;\"><span style = \"font-style: italic;\">125 rows omitted</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"header\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">SepalLength</th><th style = \"text-align: left;\">SepalWidth</th><th style = \"text-align: left;\">PetalLength</th><th style = \"text-align: left;\">PetalWidth</th><th style = \"text-align: left;\">Species</th></tr><tr class = \"subheader headerLastRow\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"CategoricalArrays.CategoricalValue{String, UInt8}\" style = \"text-align: left;\">Cat‚Ä¶</th></tr></thead><tbody><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1</td><td style = \"text-align: right;\">5.1</td><td style = \"text-align: right;\">3.5</td><td style = \"text-align: right;\">1.4</td><td style = \"text-align: right;\">0.2</td><td style = \"text-align: left;\">setosa</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">2</td><td style = \"text-align: right;\">4.9</td><td style = \"text-align: right;\">3.0</td><td style = \"text-align: right;\">1.4</td><td style = \"text-align: right;\">0.2</td><td style = \"text-align: left;\">setosa</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">3</td><td style = \"text-align: right;\">4.7</td><td style = \"text-align: right;\">3.2</td><td style = \"text-align: right;\">1.3</td><td style = \"text-align: right;\">0.2</td><td style = \"text-align: left;\">setosa</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">4</td><td style = \"text-align: right;\">4.6</td><td style = \"text-align: right;\">3.1</td><td style = \"text-align: right;\">1.5</td><td style = \"text-align: right;\">0.2</td><td style = \"text-align: left;\">setosa</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">5</td><td style = \"text-align: right;\">5.0</td><td style = \"text-align: right;\">3.6</td><td style = \"text-align: right;\">1.4</td><td style = \"text-align: right;\">0.2</td><td style = \"text-align: left;\">setosa</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">6</td><td style = \"text-align: right;\">5.4</td><td style = \"text-align: right;\">3.9</td><td style = \"text-align: right;\">1.7</td><td style = \"text-align: right;\">0.4</td><td style = \"text-align: left;\">setosa</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">7</td><td style = \"text-align: right;\">4.6</td><td style = \"text-align: right;\">3.4</td><td style = \"text-align: right;\">1.4</td><td style = \"text-align: right;\">0.3</td><td style = \"text-align: left;\">setosa</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">8</td><td style = \"text-align: right;\">5.0</td><td style = \"text-align: right;\">3.4</td><td style = \"text-align: right;\">1.5</td><td style = \"text-align: right;\">0.2</td><td style = \"text-align: left;\">setosa</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">9</td><td style = \"text-align: right;\">4.4</td><td style = \"text-align: right;\">2.9</td><td style = \"text-align: right;\">1.4</td><td style = \"text-align: right;\">0.2</td><td style = \"text-align: left;\">setosa</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">10</td><td style = \"text-align: right;\">4.9</td><td style = \"text-align: right;\">3.1</td><td style = \"text-align: right;\">1.5</td><td style = \"text-align: right;\">0.1</td><td style = \"text-align: left;\">setosa</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">11</td><td style = \"text-align: right;\">5.4</td><td style = \"text-align: right;\">3.7</td><td style = \"text-align: right;\">1.5</td><td style = \"text-align: right;\">0.2</td><td style = \"text-align: left;\">setosa</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">12</td><td style = \"text-align: right;\">4.8</td><td style = \"text-align: right;\">3.4</td><td style = \"text-align: right;\">1.6</td><td style = \"text-align: right;\">0.2</td><td style = \"text-align: left;\">setosa</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">13</td><td style = \"text-align: right;\">4.8</td><td style = \"text-align: right;\">3.0</td><td style = \"text-align: right;\">1.4</td><td style = \"text-align: right;\">0.1</td><td style = \"text-align: left;\">setosa</td></tr><tr><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">139</td><td style = \"text-align: right;\">6.0</td><td style = \"text-align: right;\">3.0</td><td style = \"text-align: right;\">4.8</td><td style = \"text-align: right;\">1.8</td><td style = \"text-align: left;\">virginica</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">140</td><td style = \"text-align: right;\">6.9</td><td style = \"text-align: right;\">3.1</td><td style = \"text-align: right;\">5.4</td><td style = \"text-align: right;\">2.1</td><td style = \"text-align: left;\">virginica</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">141</td><td style = \"text-align: right;\">6.7</td><td style = \"text-align: right;\">3.1</td><td style = \"text-align: right;\">5.6</td><td style = \"text-align: right;\">2.4</td><td style = \"text-align: left;\">virginica</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">142</td><td style = \"text-align: right;\">6.9</td><td style = \"text-align: right;\">3.1</td><td style = \"text-align: right;\">5.1</td><td style = \"text-align: right;\">2.3</td><td style = \"text-align: left;\">virginica</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">143</td><td style = \"text-align: right;\">5.8</td><td style = \"text-align: right;\">2.7</td><td style = \"text-align: right;\">5.1</td><td style = \"text-align: right;\">1.9</td><td style = \"text-align: left;\">virginica</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">144</td><td style = \"text-align: right;\">6.8</td><td style = \"text-align: right;\">3.2</td><td style = \"text-align: right;\">5.9</td><td style = \"text-align: right;\">2.3</td><td style = \"text-align: left;\">virginica</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">145</td><td style = \"text-align: right;\">6.7</td><td style = \"text-align: right;\">3.3</td><td style = \"text-align: right;\">5.7</td><td style = \"text-align: right;\">2.5</td><td style = \"text-align: left;\">virginica</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">146</td><td style = \"text-align: right;\">6.7</td><td style = \"text-align: right;\">3.0</td><td style = \"text-align: right;\">5.2</td><td style = \"text-align: right;\">2.3</td><td style = \"text-align: left;\">virginica</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">147</td><td style = \"text-align: right;\">6.3</td><td style = \"text-align: right;\">2.5</td><td style = \"text-align: right;\">5.0</td><td style = \"text-align: right;\">1.9</td><td style = \"text-align: left;\">virginica</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">148</td><td style = \"text-align: right;\">6.5</td><td style = \"text-align: right;\">3.0</td><td style = \"text-align: right;\">5.2</td><td style = \"text-align: right;\">2.0</td><td style = \"text-align: left;\">virginica</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">149</td><td style = \"text-align: right;\">6.2</td><td style = \"text-align: right;\">3.4</td><td style = \"text-align: right;\">5.4</td><td style = \"text-align: right;\">2.3</td><td style = \"text-align: left;\">virginica</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">150</td><td style = \"text-align: right;\">5.9</td><td style = \"text-align: right;\">3.0</td><td style = \"text-align: right;\">5.1</td><td style = \"text-align: right;\">1.8</td><td style = \"text-align: left;\">virginica</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccc}\n",
       "\t& SepalLength & SepalWidth & PetalLength & PetalWidth & Species\\\\\n",
       "\t\\hline\n",
       "\t& Float64 & Float64 & Float64 & Float64 & Cat‚Ä¶\\\\\n",
       "\t\\hline\n",
       "\t1 & 5.1 & 3.5 & 1.4 & 0.2 & setosa \\\\\n",
       "\t2 & 4.9 & 3.0 & 1.4 & 0.2 & setosa \\\\\n",
       "\t3 & 4.7 & 3.2 & 1.3 & 0.2 & setosa \\\\\n",
       "\t4 & 4.6 & 3.1 & 1.5 & 0.2 & setosa \\\\\n",
       "\t5 & 5.0 & 3.6 & 1.4 & 0.2 & setosa \\\\\n",
       "\t6 & 5.4 & 3.9 & 1.7 & 0.4 & setosa \\\\\n",
       "\t7 & 4.6 & 3.4 & 1.4 & 0.3 & setosa \\\\\n",
       "\t8 & 5.0 & 3.4 & 1.5 & 0.2 & setosa \\\\\n",
       "\t9 & 4.4 & 2.9 & 1.4 & 0.2 & setosa \\\\\n",
       "\t10 & 4.9 & 3.1 & 1.5 & 0.1 & setosa \\\\\n",
       "\t11 & 5.4 & 3.7 & 1.5 & 0.2 & setosa \\\\\n",
       "\t12 & 4.8 & 3.4 & 1.6 & 0.2 & setosa \\\\\n",
       "\t13 & 4.8 & 3.0 & 1.4 & 0.1 & setosa \\\\\n",
       "\t14 & 4.3 & 3.0 & 1.1 & 0.1 & setosa \\\\\n",
       "\t15 & 5.8 & 4.0 & 1.2 & 0.2 & setosa \\\\\n",
       "\t16 & 5.7 & 4.4 & 1.5 & 0.4 & setosa \\\\\n",
       "\t17 & 5.4 & 3.9 & 1.3 & 0.4 & setosa \\\\\n",
       "\t18 & 5.1 & 3.5 & 1.4 & 0.3 & setosa \\\\\n",
       "\t19 & 5.7 & 3.8 & 1.7 & 0.3 & setosa \\\\\n",
       "\t20 & 5.1 & 3.8 & 1.5 & 0.3 & setosa \\\\\n",
       "\t21 & 5.4 & 3.4 & 1.7 & 0.2 & setosa \\\\\n",
       "\t22 & 5.1 & 3.7 & 1.5 & 0.4 & setosa \\\\\n",
       "\t23 & 4.6 & 3.6 & 1.0 & 0.2 & setosa \\\\\n",
       "\t24 & 5.1 & 3.3 & 1.7 & 0.5 & setosa \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m150√ó5 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m‚îÇ\u001b[1m SepalLength \u001b[0m\u001b[1m SepalWidth \u001b[0m\u001b[1m PetalLength \u001b[0m\u001b[1m PetalWidth \u001b[0m\u001b[1m Species   \u001b[0m\n",
       "     ‚îÇ\u001b[90m Float64     \u001b[0m\u001b[90m Float64    \u001b[0m\u001b[90m Float64     \u001b[0m\u001b[90m Float64    \u001b[0m\u001b[90m Cat‚Ä¶      \u001b[0m\n",
       "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
       "   1 ‚îÇ         5.1         3.5          1.4         0.2  setosa\n",
       "   2 ‚îÇ         4.9         3.0          1.4         0.2  setosa\n",
       "   3 ‚îÇ         4.7         3.2          1.3         0.2  setosa\n",
       "   4 ‚îÇ         4.6         3.1          1.5         0.2  setosa\n",
       "   5 ‚îÇ         5.0         3.6          1.4         0.2  setosa\n",
       "   6 ‚îÇ         5.4         3.9          1.7         0.4  setosa\n",
       "   7 ‚îÇ         4.6         3.4          1.4         0.3  setosa\n",
       "   8 ‚îÇ         5.0         3.4          1.5         0.2  setosa\n",
       "  ‚ãÆ  ‚îÇ      ‚ãÆ           ‚ãÆ            ‚ãÆ           ‚ãÆ           ‚ãÆ\n",
       " 144 ‚îÇ         6.8         3.2          5.9         2.3  virginica\n",
       " 145 ‚îÇ         6.7         3.3          5.7         2.5  virginica\n",
       " 146 ‚îÇ         6.7         3.0          5.2         2.3  virginica\n",
       " 147 ‚îÇ         6.3         2.5          5.0         1.9  virginica\n",
       " 148 ‚îÇ         6.5         3.0          5.2         2.0  virginica\n",
       " 149 ‚îÇ         6.2         3.4          5.4         2.3  virginica\n",
       " 150 ‚îÇ         5.9         3.0          5.1         1.8  virginica\n",
       "\u001b[36m                                                   135 rows omitted\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "iris = dataset(\"datasets\", \"iris\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150-element CategoricalArrays.CategoricalArray{String,1,UInt8}:\n",
       " \"setosa\"\n",
       " \"setosa\"\n",
       " \"setosa\"\n",
       " \"setosa\"\n",
       " \"setosa\"\n",
       " \"setosa\"\n",
       " \"setosa\"\n",
       " \"setosa\"\n",
       " \"setosa\"\n",
       " \"setosa\"\n",
       " ‚ãÆ\n",
       " \"virginica\"\n",
       " \"virginica\"\n",
       " \"virginica\"\n",
       " \"virginica\"\n",
       " \"virginica\"\n",
       " \"virginica\"\n",
       " \"virginica\"\n",
       " \"virginica\"\n",
       " \"virginica\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = Matrix(iris[:,1:4])\n",
    "irislabels = iris[:,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150√ó4 Matrix{Float64}:\n",
       " 5.1  3.5  1.4  0.2\n",
       " 4.9  3.0  1.4  0.2\n",
       " 4.7  3.2  1.3  0.2\n",
       " 4.6  3.1  1.5  0.2\n",
       " 5.0  3.6  1.4  0.2\n",
       " 5.4  3.9  1.7  0.4\n",
       " 4.6  3.4  1.4  0.3\n",
       " 5.0  3.4  1.5  0.2\n",
       " 4.4  2.9  1.4  0.2\n",
       " 4.9  3.1  1.5  0.1\n",
       " ‚ãÆ              \n",
       " 6.9  3.1  5.1  2.3\n",
       " 5.8  2.7  5.1  1.9\n",
       " 6.8  3.2  5.9  2.3\n",
       " 6.7  3.3  5.7  2.5\n",
       " 6.7  3.0  5.2  2.3\n",
       " 6.3  2.5  5.0  1.9\n",
       " 6.5  3.0  5.2  2.0\n",
       " 6.2  3.4  5.4  2.3\n",
       " 5.9  3.0  5.1  1.8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`X` is a matrix that contains the features of the iris dataset. The iris dataset consists of measurements of 150 iris flowers from three different species. There are four features measured: sepal length, sepal width, petal length, and petal width. By doing `X = Matrix(iris[:,1:4])`, we are extracting these four features from the dataset and storing them in `X`.\n",
    "\n",
    "The reason for checking or examining `X` is to understand the structure and properties of your data before applying machine learning algorithms. This could involve checking the size of `X`, looking at the range of values for each feature, or visualizing the data. Understanding your data can help you choose appropriate machine learning methods and interpret the results correctly.\n",
    "\n",
    "In the code cell below, `irislabelsmap = labelmap(irislabels)` is creating a mapping of the labels in the data to integer values. The `labelmap` function from the `MLBase` package in Julia creates a dictionary that maps each unique value in `irislabels` to a unique integer.\n",
    "\n",
    "Then, `y = labelencode(irislabelsmap, irislabels)` is using this mapping to transform the labels into integers. The `labelencode` function takes the mapping created by `labelmap` and the original labels, and returns a vector where each original label is replaced by its corresponding integer.\n",
    "\n",
    "This is often done in machine learning when you have categorical labels. Many machine learning algorithms require numerical input and output, so it's common to encode categorical labels as integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150-element Vector{Int64}:\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " ‚ãÆ\n",
       " 3\n",
       " 3\n",
       " 3\n",
       " 3\n",
       " 3\n",
       " 3\n",
       " 3\n",
       " 3\n",
       " 3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a mapping of the labels to integers\n",
    "irislabelsmap = labelmap(irislabels)\n",
    "# irislabelsmap is now a dictionary that maps each unique label in irislabels to a unique integer\n",
    "\n",
    "# Use this mapping to transform our labels into integers\n",
    "y = labelencode(irislabelsmap, irislabels)\n",
    "# y now contains integer labels corresponding to the original labels in irislabels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In classification, we often want to use some of the data to fit a model, and the rest of the data to validate (commonly known as `training` and `testing` data). We will get this data ready now so that we can easily use it in the rest of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "perclass_splits (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function perclass_splits(y,at)\n",
    "    uids = unique(y)\n",
    "    keepids = []\n",
    "    for ui in uids\n",
    "        curids = findall(y.==ui)\n",
    "        rowids = randsubseq(curids, at) \n",
    "        push!(keepids,rowids...)\n",
    "    end\n",
    "    return keepids\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `randsubseq` function is a built-in Julia function that returns a random subsequence of a given sequence. It‚Äôs used in `perclass_splits` function to select a random subset of indices for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "randsubseq (generic function with 2 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "randsubseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35-element Vector{Any}:\n",
       "   6\n",
       "   7\n",
       "  13\n",
       "  20\n",
       "  22\n",
       "  23\n",
       "  26\n",
       "  27\n",
       "  33\n",
       "  37\n",
       "   ‚ãÆ\n",
       " 114\n",
       " 117\n",
       " 118\n",
       " 124\n",
       " 133\n",
       " 140\n",
       " 145\n",
       " 148\n",
       " 149"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# trainids are the indices of the training data\n",
    "trainids = perclass_splits(y,0.7)\n",
    "\n",
    "# testids are the indices of the test data\n",
    "testids = setdiff(1:length(y),trainids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need one more function, and that is the function that will assign classes based on the predicted values when the predicted values are continuous. This is necessary when the predicted values are continuous but the labels are discrete. The function works by finding the class label that is closest to the predicted value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assign_class (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "assign_class(predictedvalue) = argmin(abs.(predictedvalue .- [1,2,3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üü£ Method 1: Lasso\n",
    "It's a type of regression analysis method that performs both variable selection and regularization in order to enhance the prediction accuracy and interpretability of the statistical model it produces. In Julia, you can use the `glmnet` function from the `GLMNet` package to fit a generalized linear model via penalized maximum likelihood, which is the principle behind the Lasso method¬π.\n",
    "\n",
    "In your code, `path = glmnet(X[trainids,:], y[trainids])` is fitting a Lasso model to your training data. The `glmnet` function computes a path of solutions for varying penalty strength, which is useful for tuning the Lasso model¬≤.\n",
    "\n",
    "Then, `cv = glmnetcv(X[trainids,:], y[trainids])` is performing cross-validation to find the optimal penalty strength¬≥. Cross-validation is a resampling procedure used to evaluate machine learning models on a limited data sample. The procedure has a single parameter called k that refers to the number of groups that a given data sample is to be split into.\n",
    "\n",
    "References:\n",
    "1. Lasso.jl - JuliaStats. https://juliastats.org/Lasso.jl/stable/\n",
    "2. GitHub - JuliaStats/Lasso.jl: Lasso/Elastic Net linear and generalized. https://github.com/JuliaStats/Lasso.jl\n",
    "3. Introduction to Lasso Regression - Statology. https://www.statology.org/lasso-regression/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Least Squares GLMNet Cross Validation\n",
       "72 models for 4 predictors in 10 folds\n",
       "Best Œª 0.002 (mean loss 0.054, std 0.007)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fit a Lasso model to the training data\n",
    "path = glmnet(X[trainids,:], y[trainids])\n",
    "\n",
    "# Perform cross-validation to find the optimal penalty strength\n",
    "cv = glmnetcv(X[trainids,:], y[trainids])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output indicates that cross-validation was performed with 72 models for 4 predictors in 10 folds. The best lambda (Œª) value found was 0.002, which resulted in a mean loss of 0.054 and a standard deviation of 0.007.\n",
    "\n",
    "This way, you're preparing your data correctly for the machine learning algorithms you'll use later on! üòä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the best lambda to predict with.\n",
    "path = glmnet(X[trainids,:], y[trainids])\n",
    "cv = glmnetcv(X[trainids,:], y[trainids])\n",
    "mylambda = path.lambda[argmin(cv.meanloss)]\n",
    "\n",
    "path = glmnet(X[trainids,:], y[trainids],lambda=[mylambda]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35√ó1 Matrix{Float64}:\n",
       " 1.0980319067139004\n",
       " 1.023125473670924\n",
       " 0.9268306739701447\n",
       " 1.010077648257915\n",
       " 1.0801925512521762\n",
       " 0.8807193545134296\n",
       " 1.022107433474087\n",
       " 1.1184022331748664\n",
       " 0.8625253412214363\n",
       " 0.9367683918978442\n",
       " ‚ãÆ\n",
       " 2.7412172522271927\n",
       " 2.660230836988896\n",
       " 3.047726581736285\n",
       " 2.5847452674598523\n",
       " 2.9422876256488446\n",
       " 2.8250433630010314\n",
       " 3.11029450502679\n",
       " 2.7370891045450465\n",
       " 2.928660663749768"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "q = X[testids,:];\n",
    "\n",
    "# The predict function takes the fitted model and the features of the testing data, and returns predicted values.\n",
    "predictions_lasso = GLMNet.predict(path,q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The predicted values are continuous but the labels are discrete\n",
    "# assign a class label to each predicted value\n",
    "predictions_lasso = assign_class.(predictions_lasso)\n",
    "findaccuracy(predictions_lasso,y[testids])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result suggests that the model is effectively capturing patterns in the data and could be used to make reliable predictions on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üü£ Method 2: Ridge\n",
    "Ridge regression is a method for improving the conditioning of the problem and reducing the variance of the estimates. In Ridge regression, the penalty term is the L2 norm (the square root of the sum of the squares of the coefficients), while in Lasso regression, it‚Äôs the L1 norm (the sum of the absolute values of the coefficients). This leads to different properties for each method¬π.\n",
    "\n",
    "Ridge regression is a method of estimating the coefficients of multiple-regression models in scenarios where the independent variables are highly correlated¬π. It is particularly useful to mitigate the problem of multicollinearity in linear regression, which commonly occurs in models with large numbers of parameters¬π.\n",
    "\n",
    "In Ridge regression, a shrinkage penalty is introduced to the sum of squared residuals (RSS) which is minimized in ordinary least squares regression¬≤. This penalty term is controlled by a parameter lambda (Œª), and it has the effect of shrinking the estimated coefficients towards zero¬≤. When Œª = 0, Ridge regression produces the same coefficient estimates as least squares. However, as Œª approaches infinity, the shrinkage penalty becomes more influential and the Ridge regression coefficient estimates approach zero¬≤.\n",
    "\n",
    "The advantage of Ridge regression compared to least squares regression lies in the bias-variance tradeoff¬≤. The basic idea of Ridge regression is to introduce a little bias so that the variance can be substantially reduced, which leads to a lower overall mean squared error (MSE)¬≤.\n",
    "\n",
    "Ridge regression is often used when the number of predictor variables in a set exceeds the number of observations, or when a data set has multicollinearity (correlations between predictor variables)¬≥. It has been used in many fields including econometrics, chemistry, and engineering¬π.\n",
    "\n",
    "References:\n",
    "1. Ridge regression - Wikipedia. https://en.wikipedia.org/wiki/Ridge_regression\n",
    "2. Introduction to Ridge Regression - Statology. https://www.statology.org/ridge-regression/\n",
    "3. Ridge Regression: Simple Definition - Statistics How To. https://www.statisticshowto.com/ridge-regression/\n",
    "4. Ridge Regression Definition & Examples | What is Ridge Regression?. https://www.mygreatlearning.com/blog/what-is-ridge-regression/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "We will use the same function but set alpha to zero.\n",
    "\n",
    "Tips: \n",
    "- Check the previous three cell\n",
    "- Add `alpha=0` in the `glmnet` and `glmnetcv` functions. This sets the elastic net mixing parameter to 0, which corresponds to Ridge regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Choose the best lambda to predict with\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üü£ Method 3: Elastic Net\n",
    "\n",
    "**Elastic Net** is a regularized regression method that linearly combines the L1 and L2 penalties of the Lasso and Ridge methods¬π. It is particularly useful when there are multiple correlated features¬≤¬≥.\n",
    "\n",
    "In more detail, Elastic Net is a compromise between Lasso regression (which uses an L1 penalty) and Ridge regression (which uses an L2 penalty). The Elastic Net algorithm uses both L1 and L2 penalties, effectively combining properties of both Lasso and Ridge¬π¬≤.\n",
    "\n",
    "The key advantage of Elastic Net over Lasso is that it can select groups of correlated variables, whereas Lasso tends to select one variable from each group and ignore the others¬π. This makes Elastic Net particularly useful when dealing with high-dimensional data where predictors are highly correlated.\n",
    "\n",
    "References:\n",
    "1. Elastic net regularization - Wikipedia. https://en.wikipedia.org/wiki/Elastic_net_regularization\n",
    "2. Scikit Learn - Elastic-Net - Online Tutorials Library. https://www.tutorialspoint.com/scikit_learn/scikit_learn_elastic_net.htm\n",
    "3. Elastic Net Regression Explained, Step by Step - Machine Learning Compass. https://machinelearningcompass.com/machine_learning_models/elastic_net_regression/\n",
    "4. Elastic Net - Overview, Geometry, and Regularization. https://corporatefinanceinstitute.com/resources/data-science/elastic-net/\n",
    "\n",
    "The Elastic Net method includes the LASSO and Ridge regression: in other words, each of them is a special case where Œ±=1 or Œ±=0, respectively. Here, Œ± is the mixing parameter between Ridge (Œ±=0) and Lasso (Œ±=1). If Œ±=0.5, it's an equal mix of Ridge and Lasso¬π.\n",
    "We will use the same function as before but set alpha to 0.5 (it's the combination of lasso and ridge).\n",
    "We fit the model to the training data, perform cross-validation to find the optimal penalty strength, make predictions on the testing data, and then calculate the accuracy of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Choose the best lambda to predict with\n",
    "path = glmnet(X[trainids,:], y[trainids],alpha=0.5);\n",
    "cv = glmnetcv(X[trainids,:], y[trainids],alpha=0.5)\n",
    "mylambda = path.lambda[argmin(cv.meanloss)]\n",
    "path = glmnet(X[trainids,:], y[trainids],alpha=0.5,lambda=[mylambda]);\n",
    "q = X[testids,:];\n",
    "predictions_EN = GLMNet.predict(path,q)\n",
    "predictions_EN = assign_class.(predictions_EN)\n",
    "findaccuracy(predictions_EN,y[testids])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üü£ Method 4: Decision Trees\n",
    "**Decision Trees** are a type of non-parametric supervised learning method used for both classification and regression tasks¬π¬≤. The goal of a decision tree is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features¬≥‚Å¥. Detailed explanation:\n",
    "\n",
    "1. **Structure**: A decision tree is composed of nodes, which include root nodes, internal nodes, and leaf nodes¬π. A root node has outgoing edges but no incoming edges. An internal node represents a feature in our data which is used to split the data into two or more children nodes. At each internal node, a decision is made based on the feature to determine which child node to proceed to. A leaf node represents the outcome (or class label in classification tasks) and contains no outgoing edges¬π.\n",
    "\n",
    "2. **Decision Making**: The decisions at each node are made based on certain conditions on the features. For example, if we have a feature 'Age', an internal node could have a condition like 'Age < 18', and the tree would route data instances based on whether this condition is true or false¬≤.\n",
    "\n",
    "3. **Learning**: The process of learning a decision tree from data involves determining the best feature to split on at each node, and this is often done using measures such as information gain or Gini impurity¬≤.\n",
    "\n",
    "4. **Prediction**: To make a prediction for a new instance, you start at the root of the tree and follow the appropriate path through the internal nodes until you reach a leaf node. The prediction of the decision tree is then the value associated with that leaf node¬≤.\n",
    "\n",
    "5. **Advantages**: Decision trees are popular because they're easy to understand and interpret, can handle both numerical and categorical data, and can handle multi-output problems¬≤.\n",
    "\n",
    "6. **Disadvantages**: They can create over-complex trees that overfit the data and can be unstable because small variations in the data might result in a completely different tree being generated¬≤.\n",
    "\n",
    "References:\n",
    "1. Decision Tree Algorithm - TowardsMachineLearning. https://towardsmachinelearning.org/decision-tree-algorithm/\n",
    "2. Decision Trees ‚Äî scikit-learn 1.3.1 documentation. https://scikit-learn.org/stable/modules/tree.html\n",
    "3. Decision tree - Wikipedia. https://en.wikipedia.org/wiki/Decision_tree\n",
    "4. Decision tree learning - Wikipedia. https://en.wikipedia.org/wiki/Decision_tree_learning\n",
    "5. Decision Tree - Overview, Decision Types, Applications. https://corporatefinanceinstitute.com/resources/data-science/decision-tree/\n",
    "\n",
    "We will use the package `DecisionTree`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier\n",
       "max_depth:                2\n",
       "min_samples_leaf:         1\n",
       "min_samples_split:        2\n",
       "min_purity_increase:      0.0\n",
       "pruning_purity_threshold: 1.0\n",
       "n_subfeatures:            0\n",
       "classes:                  [1, 2, 3]\n",
       "root:                     Decision Tree\n",
       "Leaves: 3\n",
       "Depth:  2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = DecisionTreeClassifier(max_depth=2)\n",
    "DecisionTree.fit!(model, X[trainids,:], y[trainids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "q = X[testids,:];\n",
    "predictions_DT = DecisionTree.predict(model, q)\n",
    "findaccuracy(predictions_DT,y[testids])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üü£ Method 5: Random Forests\n",
    "**Random Forests** is a powerful machine learning technique that's used for both regression and classification problems¬π¬≤¬≥. It's an ensemble learning method, which means it combines the predictions of multiple machine learning algorithms to make more accurate predictions‚Å¥. Detailed explanation:\n",
    "\n",
    "1. **Ensemble of Decision Trees**: A Random Forest is essentially a collection of decision trees, each trained on a random subset of the data¬π¬≤‚Å¥. Each tree in the forest is built from a sample drawn with replacement (i.e., a bootstrap sample) from the training set¬≥.\n",
    "\n",
    "2. **Feature Randomness**: In addition, when splitting each node during the construction of a tree, the best split is found either from all input features or a random subset of size `m` (where `m` is specified by the user)¬≥.\n",
    "\n",
    "3. **Prediction**: For a classification problem, the output of the Random Forest is the class selected by most trees (majority vote). For regression problems, it's typically the average prediction of all trees¬π¬≤.\n",
    "\n",
    "4. **Advantages**: Random Forests correct for decision trees' habit of overfitting to their training set¬π. They generally outperform decision trees and are robust to outliers and non-linear data¬π¬≤. They also handle large datasets with high dimensionality well‚Åµ.\n",
    "\n",
    "5. **Disadvantages**: However, they're not easily interpretable and can be computationally intensive depending on the number of trees and depth¬≤.\n",
    "\n",
    "References:\n",
    "1. Random forest - Wikipedia. https://en.wikipedia.org/wiki/Random_forest.\n",
    "2. What is Random Forest? | IBM. https://www.ibm.com/topics/random-forest\n",
    "3. Introduction to Random Forest in Machine Learning. https://www.section.io/engineering-education/introduction-to-random-forest-in-machine-learning/\n",
    "4. Random Forest - Overview, Modeling Predictions, Advantages. https://corporatefinanceinstitute.com/resources/data-science/random-forest/\n",
    "5. Random Forests ‚Äì SpringerLink. https://link.springer.com/article/10.1023/A:1010933404324"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Use Random Forests in a similar way as you used Ridge, Lasso, Elastic Net, and Decision Trees. \n",
    "\n",
    "Tips: \n",
    "- Check the previous cells\n",
    "- The `RandomForestClassifier` is available through the `DecisionTree` package as well\n",
    "- n_trees=20\n",
    "\n",
    "The `n_trees` parameter in the Random Forest algorithm refers to the number of trees that will be grown in the forest. Each of these trees will be trained on a different subset of the data and will make its own predictions. The final prediction of the Random Forest is then based on the majority vote (for classification) or average (for regression) of the predictions of all the trees.\n",
    "\n",
    "The choice of `n_trees=20` is somewhat arbitrary and is often chosen based on empirical results. In general, having more trees in the forest can increase the model's performance because it allows for more diverse subsets of the data to be considered. However, after a certain point, adding more trees can lead to diminishing returns in terms of accuracy improvement, and can also increase computational cost.\n",
    "\n",
    "It's often a good idea to experiment with different values of `n_trees` and use cross-validation to find the value that gives the best performance on your specific dataset. In practice, values in the range of 100-1000 are commonly used, but smaller values like 20 can also give good results depending on the dataset and problem at hand. üòä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "q = X[testids,:];\n",
    "predictions_RF = DecisionTree.predict(model, q)\n",
    "findaccuracy(predictions_RF,y[testids])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üü£ Method 6: Using a Nearest Neighbor method\n",
    "Nearest Neighbor method is a type of proximity search optimization problem that aims to find the point in a given set that is closest or most similar to a given point. This method is often used in machine learning for both classification and regression tasks. Nearest Neighbor method for classification is method based on the principle that instances within a dataset will generally exist in close proximity to other instances that have similar properties¬π.\n",
    "\n",
    "First, we're creating a KDTree, a space-partitioning data structure for organizing points in a K-dimensional space, with the training data using `kdtree = KDTree(Xtrain')`. The KDTree will allow efficient spatial queries for the nearest neighbors¬≤.\n",
    "\n",
    "Next, we're defining the queries, which are the data points you want to classify, with `queries = X[testids,:]`.\n",
    "\n",
    "Then, we're finding the 5 nearest neighbors of each query point in the KDTree with `idxs, dists = knn(kdtree, queries', 5, true)`. The `knn` function returns two arrays: `idxs` contains the indices of the 5 nearest neighbors of each query point, and `dists` contains the distances to these neighbors¬≤.\n",
    "\n",
    "After that, we're getting the labels of the nearest neighbors with `c = ytrain[hcat(idxs...)]`.\n",
    "\n",
    "Then, we're counting the number of occurrences of each label in the nearest neighbors of each query point with `possible_labels = map(i->counter(c[:,i]),1:size(c,2))`. The `counter` function returns a dictionary where the keys are the unique labels and the values are their counts¬≤.\n",
    "\n",
    "Next, we're assigning each query point to the class that occurs most frequently among its 5 nearest neighbors with `predictions_NN = map(i->parse(Int,string(string(argmax(possible_labels[i])))),1:size(c,2))`. If there's a tie for the most frequent class, `argmax` will return one of them at random¬≤.\n",
    "\n",
    "Finally, we're calculating the accuracy of your predictions with `findaccuracy(predictions_NN,y[testids])`. The output shows that the model achieved an accuracy of some % on the testing data. \n",
    "\n",
    "References:\n",
    "1. Neighborhood.jl: Unified API for finding nearest neighbors in Julia. https://discourse.julialang.org/t/neighborhood-jl-unified-api-for-finding-nearest-neighbors-in-julia/38659\n",
    "2. NearestNeighbors ¬∑ Julia Packages. https://juliapackages.com/p/nearestneighbors\n",
    "3. GitHub - KristofferC/NearestNeighbors.jl: High performance nearest. https://github.com/KristofferC/NearestNeighbors.jl\n",
    "4. GitHub - JuliaAI/NearestNeighborModels.jl: Package providing K-nearest. https://github.com/JuliaAI/NearestNeighborModels.jl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KDTree{StaticArraysCore.SVector{4, Float64}, Euclidean, Float64}\n",
       "  Number of points: 115\n",
       "  Dimensions: 4\n",
       "  Metric: Euclidean(0.0)\n",
       "  Reordered: true"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Xtrain = X[trainids,:]\n",
    "ytrain = y[trainids]\n",
    "kdtree = KDTree(Xtrain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35√ó4 Matrix{Float64}:\n",
       " 5.4  3.9  1.7  0.4\n",
       " 4.6  3.4  1.4  0.3\n",
       " 4.8  3.0  1.4  0.1\n",
       " 5.1  3.8  1.5  0.3\n",
       " 5.1  3.7  1.5  0.4\n",
       " 4.6  3.6  1.0  0.2\n",
       " 5.0  3.0  1.6  0.2\n",
       " 5.0  3.4  1.6  0.4\n",
       " 5.2  4.1  1.5  0.1\n",
       " 5.5  3.5  1.3  0.2\n",
       " ‚ãÆ              \n",
       " 5.7  2.5  5.0  2.0\n",
       " 6.5  3.0  5.5  1.8\n",
       " 7.7  3.8  6.7  2.2\n",
       " 6.3  2.7  4.9  1.8\n",
       " 6.4  2.8  5.6  2.2\n",
       " 6.9  3.1  5.4  2.1\n",
       " 6.7  3.3  5.7  2.5\n",
       " 6.5  3.0  5.2  2.0\n",
       " 6.2  3.4  5.4  2.3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "queries = X[testids,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[16, 9, 35, 33, 14], [3, 10, 22, 4, 28], [2, 8, 34, 26, 23], [35, 5, 15, 1, 20], [15, 5, 35, 1, 20], [3, 28, 31, 5, 27], [26, 8, 2, 23, 34], [18, 6, 30, 15, 10], [25, 35, 9, 14, 5], [9, 24, 21, 35, 20]  ‚Ä¶  [99, 114, 80, 107, 103], [111, 93, 88, 63, 115], [107, 80, 99, 87, 86], [102, 81, 85, 105, 94], [97, 114, 98, 54, 103], [99, 80, 87, 109, 89], [87, 113, 110, 92, 109], [109, 92, 112, 95, 106], [86, 113, 89, 107, 58], [106, 89, 86, 109, 107]], [[0.33166247903553986, 0.3464101615137753, 0.3605551275463989, 0.3741657386773947, 0.3999999999999999], [0.264575131106459, 0.3000000000000002, 0.31622776601683805, 0.33166247903553986, 0.4123105625617666], [0.1414213562373099, 0.17320508075688815, 0.19999999999999998, 0.20000000000000037, 0.244948974278318], [0.2449489742783178, 0.26457513110645875, 0.31622776601683783, 0.33166247903553986, 0.33166247903553997], [0.244948974278318, 0.26457513110645897, 0.2828427124746191, 0.30000000000000016, 0.3000000000000003], [0.5099019513592785, 0.5099019513592788, 0.5196152422706636, 0.5656854249492382, 0.6000000000000002], [0.17320508075688762, 0.19999999999999993, 0.22360679774997896, 0.22360679774997916, 0.3000000000000002], [0.1999999999999998, 0.22360679774997902, 0.24494897427831772, 0.2645751311064591, 0.2828427124746191], [0.3464101615137755, 0.42426406871192796, 0.4582575694955836, 0.458257569495584, 0.5567764362830019], [0.3, 0.31622776601683783, 0.3316624790355398, 0.3464101615137756, 0.3605551275463988]  ‚Ä¶  [0.374165738677394, 0.3741657386773942, 0.38729833462074154, 0.45825756949558394, 0.469041575982343], [0.2645751311064589, 0.33166247903553986, 0.5196152422706629, 0.547722557505166, 0.5830951894845301], [0.1414213562373093, 0.24494897427831783, 0.3872983346207416, 0.42426406871192845, 0.48989794855663593], [0.4123105625617661, 0.818535277187245, 0.860232526704263, 1.0049875621120892, 1.019803902718557], [0.17320508075688762, 0.24494897427831774, 0.3605551275463989, 0.36055512754639907, 0.3741657386773937], [0.10000000000000009, 0.4242640687119288, 0.46904157598234253, 0.4690415759823429, 0.5099019513592786], [0.1732050807568879, 0.360555127546399, 0.36055512754639935, 0.37416573867739383, 0.4123105625617657], [0.24494897427831785, 0.30000000000000016, 0.31622776601683794, 0.3999999999999999, 0.43588989435406783], [0.22360679774997935, 0.3605551275463989, 0.3872983346207415, 0.3872983346207415, 0.4123105625617663], [0.2449489742783171, 0.30000000000000016, 0.5567764362830023, 0.6244997998398395, 0.6244997998398396]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idxs, dists = knn(kdtree, queries', 5, true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "c = ytrain[hcat(idxs...)]\n",
    "possible_labels = map(i->counter(c[:,i]),1:size(c,2))\n",
    "predictions_NN = map(i->parse(Int,string(string(argmax(possible_labels[i])))),1:size(c,2))\n",
    "findaccuracy(predictions_NN,y[testids])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üü£ Method 7: Support Vector Machines\n",
    "**Support Vector Machines (SVM)** is a supervised machine learning model that uses classification algorithms for two-group classification problems¬π. After giving an SVM model sets of labeled training data for each category, they're able to categorize new text¬≤. Detailed explanation:\n",
    "\n",
    "1. **Classification and Regression**: SVMs are a set of supervised learning methods used for classification, regression, and outliers detection.\n",
    "\n",
    "2. **High Dimensionality**: They are effective in high dimensional spaces and still effective in cases where the number of dimensions is greater than the number of samples¬≤.\n",
    "\n",
    "3. **Memory Efficiency**: SVMs use a subset of training points in the decision function (called support vectors), so it is also memory efficient¬≤.\n",
    "\n",
    "4. **Versatility**: Different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.\n",
    "\n",
    "5. **Disadvantages**: If the number of features is much greater than the number of samples, avoiding over-fitting in choosing Kernel functions and regularization term is crucial. SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation‚Å¥.\n",
    "\n",
    "References:\n",
    "1. Support Vector Machines (SVM) Algorithm Explained - MonkeyLearn. https://monkeylearn.com/blog/introduction-to-support-vector-machines-svm/\n",
    "2. Support Vector Machines ‚Äî scikit-learn 1.3.1 documentation. https://scikit-learn.org/stable/modules/svm.html\n",
    "3. Support vector machine - Wikipedia. https://en.wikipedia.org/wiki/Support_vector_machine\n",
    "4. Introduction to Support Vector Machines (SVM) - GeeksforGeeks. https://www.geeksforgeeks.org/introduction-to-support-vector-machines-svm/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115-element Vector{Int64}:\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " ‚ãÆ\n",
       " 3\n",
       " 3\n",
       " 3\n",
       " 3\n",
       " 3\n",
       " 3\n",
       " 3\n",
       " 3\n",
       " 3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Xtrain = X[trainids,:]\n",
    "ytrain = y[trainids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LIBSVM.SVM{Int64, LIBSVM.Kernel.KERNEL}(SVC, LIBSVM.Kernel.RadialBasis, nothing, 4, 115, 3, [1, 2, 3], Int32[1, 2, 3], Float64[], Int32[], LIBSVM.SupportVectors{Vector{Int64}, Matrix{Float64}}(42, Int32[7, 18, 17], [1, 1, 1, 1, 1, 1, 1, 2, 2, 2  ‚Ä¶  3, 3, 3, 3, 3, 3, 3, 3, 3, 3], [4.3 5.7 ‚Ä¶ 6.3 5.9; 3.0 4.4 ‚Ä¶ 2.5 3.0; 1.1 1.5 ‚Ä¶ 5.0 5.1; 0.1 0.4 ‚Ä¶ 1.9 1.8], Int32[11, 13, 16, 18, 19, 32, 33, 38, 39, 40  ‚Ä¶  98, 100, 102, 103, 104, 108, 110, 111, 114, 115], LIBSVM.SVMNode[LIBSVM.SVMNode(1, 4.3), LIBSVM.SVMNode(1, 5.7), LIBSVM.SVMNode(1, 5.7), LIBSVM.SVMNode(1, 5.1), LIBSVM.SVMNode(1, 4.8), LIBSVM.SVMNode(1, 4.5), LIBSVM.SVMNode(1, 5.1), LIBSVM.SVMNode(1, 6.9), LIBSVM.SVMNode(1, 6.5), LIBSVM.SVMNode(1, 5.7)  ‚Ä¶  LIBSVM.SVMNode(1, 6.1), LIBSVM.SVMNode(1, 7.2), LIBSVM.SVMNode(1, 7.9), LIBSVM.SVMNode(1, 6.3), LIBSVM.SVMNode(1, 6.1), LIBSVM.SVMNode(1, 6.0), LIBSVM.SVMNode(1, 6.9), LIBSVM.SVMNode(1, 5.8), LIBSVM.SVMNode(1, 6.3), LIBSVM.SVMNode(1, 5.9)]), 0.0, [0.0 0.010690561607055334; 0.4454341858335524 0.9559663377862481; ‚Ä¶ ; -0.0 -1.0; -0.0 -1.0], Float64[], Float64[], [0.026936675369851212, 0.16778134297120267, 0.20843562223012754], 3, 0.25, 200.0, 0.001, 1.0, 0.5, 0.1, true, false)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = svmtrain(Xtrain', ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions_SVM, decision_values = svmpredict(model, X[testids,:]')\n",
    "findaccuracy(predictions_SVM,y[testids])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting all the results together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7√ó2 Matrix{Any}:\n",
       " \"lasso\"  1.0\n",
       " \"ridge\"  1.0\n",
       " \"EN\"     1.0\n",
       " \"DT\"     1.0\n",
       " \"RF\"     1.0\n",
       " \"kNN\"    1.0\n",
       " \"SVM\"    1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "overall_accuracies = zeros(7)\n",
    "methods = [\"lasso\",\"ridge\",\"EN\", \"DT\", \"RF\",\"kNN\", \"SVM\"]\n",
    "ytest = y[testids]\n",
    "overall_accuracies[1] = findaccuracy(predictions_lasso,ytest)\n",
    "overall_accuracies[2] = findaccuracy(predictions_ridge,ytest)\n",
    "overall_accuracies[3] = findaccuracy(predictions_EN,ytest)\n",
    "overall_accuracies[4] = findaccuracy(predictions_DT,ytest)\n",
    "overall_accuracies[5] = findaccuracy(predictions_RF,ytest)\n",
    "overall_accuracies[6] = findaccuracy(predictions_NN,ytest)\n",
    "overall_accuracies[7] = findaccuracy(predictions_SVM,ytest)\n",
    "hcat(methods, overall_accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally...\n",
    "After finishing this notebook, you should be able to:\n",
    "- [ ] split your data into training and testing data to test the effectiveness of a certain method\n",
    "- [ ] apply a simple accuracy function to test the effectiveness of a certain method\n",
    "- [ ] run multiple classification algorithms:\n",
    "    - [ ] LASSO\n",
    "    - [ ] Ridge\n",
    "    - [ ] ElasticNet\n",
    "    - [ ] Decision Tree\n",
    "    - [ ] Random Forest\n",
    "    - [ ] Nearest Neighbors\n",
    "    - [ ] Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü•≥ One cool finding\n",
    "\n",
    "We used multiple methods to run classification on the `iris` dataset which is a dataset of flowers and there are three types of iris flowers in it. We split the data into training and testing and ran our methods. Here is the scoreboard:\n",
    "\n",
    "| method | accuracy score |\n",
    "|---|---|\n",
    "| lasso  |1.0|\n",
    "| ridge  |1.0|\n",
    "| EN     |1.0|\n",
    "| DT     |0.960784|\n",
    "| RF     |0.980392|\n",
    "| kNN    |1.0|\n",
    "| SVM    |1.0|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solutions\n",
    "# Exercise 1\n",
    "path = glmnet(X[trainids,:], y[trainids],alpha=0);\n",
    "cv = glmnetcv(X[trainids,:], y[trainids],alpha=0)\n",
    "mylambda = path.lambda[argmin(cv.meanloss)]\n",
    "path = glmnet(X[trainids,:], y[trainids],alpha=0,lambda=[mylambda]);\n",
    "q = X[testids,:];\n",
    "predictions_ridge = GLMNet.predict(path,q)\n",
    "predictions_ridge = assign_class.(predictions_ridge)\n",
    "findaccuracy(predictions_ridge,y[testids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier\n",
       "n_trees:             20\n",
       "n_subfeatures:       -1\n",
       "partial_sampling:    0.7\n",
       "max_depth:           -1\n",
       "min_samples_leaf:    1\n",
       "min_samples_split:   2\n",
       "min_purity_increase: 0.0\n",
       "classes:             [1, 2, 3]\n",
       "ensemble:            Ensemble of Decision Trees\n",
       "Trees:      20\n",
       "Avg Leaves: 7.35\n",
       "Avg Depth:  4.75"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Exercise 2\n",
    "model = RandomForestClassifier(n_trees=20)\n",
    "DecisionTree.fit!(model, X[trainids,:], y[trainids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
